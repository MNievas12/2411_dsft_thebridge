{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prostate cancer prediction \n",
    "\n",
    "#### Data\n",
    "\n",
    "We consider a medical study conducted on 97 men with prostate cancer.\n",
    "The focus is on the relationship between the prostate specific antigen (psa), which is elevated in men with prostate cancer, and others clinical measures. \n",
    "The others clinical measures are the predictors variables, gathered in a medical examination, and the amount of expression of the antigen associated with the cancer detection is the response variable (lpsa).\n",
    "\n",
    "Thus the data frame is made of 97 observations on 9 variables:\n",
    "* lcavol: log cancer volume\n",
    "* lweight: log prostate weight\n",
    "* age: patient age in years\n",
    "* lbph: log amount of benign prostatic hyperplasia\n",
    "* svi: seminal vesicle invasion\n",
    "* lcp: log of capsular penetration\n",
    "* gleason: Gleason score\n",
    "* pgg45: percent of Gleason score 4 or 5\n",
    "* lpsa: log prostate specific antigen\n",
    "\n",
    "The goal is to find models predicting the response lpsa.\n",
    "\n",
    "#### Models\n",
    "\n",
    "The data are represented by $n$ points in $p$ dimensions, thus the predictor variable is written $X\\in\\mathbb{R}^{n\\times p}$ and the response variable is $y\\in\\mathbb{R}^n$.\n",
    "\n",
    "In this work, we're insterested in the relationship between the predictor $X$ and the response $y$.\n",
    "To determine this relationship, we adopt regression models.\n",
    "The standard baseline is achieved with linear regression and we compare results for regularized regressions: **Ridge regression**, **Lasso** and **Elastic Net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lpsa is almost normally distributed.\n",
    "* the presence of svi is binary\n",
    "* lcp: due to not proper measurements, for small values of capsular penetration, it has been arbitrarily set to -1.25.\n",
    "* gleason and pgg45 don't seem to be correlated...\n",
    "\n",
    "Let see the correlation between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The more correlated variable with the response lpsa is lcavol.\n",
    "  Thus in a data analysis, the lcavol variable must be included as a predictor.\n",
    "\n",
    "* The correlation matrix shows that gleason and pgg45 are actually correlated. \n",
    "  Indeed, the variable pgg45 measures the percentage of 4 or 5 Gleason scores that were recorded before the final current Gleason score.\n",
    "\n",
    "Let plot the relationship between the response lpsa and the lcavol feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a pretty clear linear relationship with positive correlation, as seen on the correlation matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning models\n",
    "\n",
    "\n",
    "# 1) Linear regression baseline\n",
    "\n",
    "The linear regression attemps to model the relationship between the predictors variables $X$ and the response variable $y$.\n",
    "It consists in finding a linear function $f:\\mathbb{R}^p \\to \\mathbb{R}$ which predicts the response $y_i$ from the predictors $X_{i1},...,X_{ip}$ given $n$ observations for $i=1,...,n$.\n",
    "\n",
    "In Python, the linear regression is implemented as  [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) in the linear_model module of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Regularization\n",
    "\n",
    "In order to avoid over-learning, the regularization method allows to control the model complexity.\n",
    "The model minimizes the error plus a regularization term $\\lambda Reg(\\beta)$ measuring the complexity, where $Reg(\\beta)$ is a penalty term and $\\lambda$ is an hyper-parameter.\n",
    "The hyper-parameter controls the relative influence of the error term and the amount of regularization.\n",
    "The optimal value of $\\lambda$ can be found by cross validation (see repository [cross-validation](https://github.com/christelle-git/cross-validation/)). \n",
    "\n",
    "## 2.1) Ridge regression \n",
    "\n",
    "In the Ridge regression, the regularization term is $Reg(\\beta) = ||\\beta||_2^2$.\n",
    "The Ridge regression allows to reduce the magnitude of the weights $\\beta_i$ of the linear regression, and thus avoid over-learning.\n",
    "The Ridge regression has a grouped selection effect: the correlated variables have the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal value of the regularization coefficient $\\lambda$ is around {...}.\n",
    "* For $\\lambda \\to 0$ the regularization terms vanishes leading to the same result as linear regression.\n",
    "* For $\\lambda \\to \\infty$ the regularization term magnitude dominates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ridge regression restricts somes variables by reducing their weights magnitude."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "The following method goes further by selecting some variables to be removed from the Ridge regression, thus reducing the dimension.\n",
    "The method is called Least Absolute Shrinkage and Selection Operator (Lasso) and the resulting simplified model is a **sparse model** or parsimonious model.\n",
    "In the Lasso, the regularization term is defined by $Reg(\\beta) = ||\\beta||_1$.\n",
    "\n",
    "The Lasso performs a model's feature selection: for correlated variables, it retains only one variable and sets other correlated variables to zero.\n",
    "The counterpart is that it obviously induces a loss of information resulting in lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal value of the regularization coefficient $\\lambda$ is between {...}.\n",
    "* For $\\lambda \\to 0$ the regularization terms vanishes so the Lasso regression tends to the linear regression.\n",
    "* For $\\lambda \\to \\infty$ the regularization term magnitude dominates the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Lasso removes somes variables by putting their weight to zero.<br>\n",
    "  This is the case if two variables are correlated.\n",
    "* As $\\lambda \\to \\infty$ weights vanish so the model becomes very **sparse**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Elastic net\n",
    "\n",
    "The Elastic Net method is a hybrid of the Ridge regression and the Lasso, thus overcomes the issue of losing information.\n",
    "The regularization term combines both the $L_1$ and the $L_2$ regularizations.\n",
    "More precisely, the regularisation term is set to $Reg(\\beta) = \\lambda((1-\\alpha)||\\beta||_1+\\alpha||\\beta||_2^2)$ where $\\alpha$ is an additional parameter to fit.\n",
    "\n",
    "The Elastic net has a selecting effect on variables as Lasso but keep correlated variables as Ridge regression.\n",
    "Thus the Elastic net model is less sparse than the Lasso, keeping more information. \n",
    "However the model is more demanding in computational resources.\n",
    "\n",
    "In what follows we present results for $\\alpha=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal value of the regularization coefficient $\\lambda$ is between $10^{-2}$ and $10^{-1}$.\n",
    "* For $\\lambda \\to 0$ the regularization terms vanishes leading to the same result as linear regression.\n",
    "* For $\\lambda \\to \\infty$ the regularization term magnitude dominates the error which is smaller than with the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As expected the Elastic net keeps more variables than the Lasso.\n",
    "* Better performance can be obtained by varying the hyper-parameter $\\alpha$ value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "# Model selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Lasso performs better than others methods (Elastic net: $\\alpha=0.5$). \n",
    "* The Lasso is more parsimonious but there is likely to be a loss of accuracy.\n",
    "* The Elastic net performs better than the Ridge regression (with $\\alpha=0.5$).\n",
    "* The Elastic net can be tuned to outperform Lasso but it is more demanding in computational resources.\n",
    "\n",
    "**=> The Elastic net is a good trade-off for accuracy and computational cost balance between the Ridge regression and the Lasso**.\n",
    "\n",
    "\n",
    "In order to optimize the model by fitting the optimal parameters, a cross validation can be performed.\n",
    "The functions sklearn.linear_model.RidgeCV, sklearn.linear_model.LassoCV and sklearn.linear_model.ElasticNetCV in Python perform an automatic tunning of hyperparameters for the Rigde regression, the Lasso and the Elastic Net respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
