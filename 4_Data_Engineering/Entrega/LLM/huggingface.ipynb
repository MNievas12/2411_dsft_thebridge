{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagina que estás mirando una película de \"Friends,\" donde querías entender cómo funciona la dinámica de la vida cotidiana de seis amigos en un apartamento de Nueva York. Ahora, supongamos que tomas notas en extremo sobre cada chiste, interacción y momento puntual de la serie, incluso aquellos que son raros o muy específicos de solo una ocasión única. Luego, intentas usar esas notas complejas para predecir los comportamientos de tus amigos en cualquier situación fuera de la escena de la firma del apartamento.\n",
      "\n",
      "Tu mala interpretación exagerada, respaldada por esas notas detalladas pero desequilibradas, se asemeja al concepto de overfitting en aprendizaje automático. Así como has construido un modelo (estudio de comportamiento) que funciona maravillosamente solo con los datos (episodios específicos de \"Friends\") pero no logra generalizar a situaciones reales (interacciones del mundo fuera del programa), un modelo de aprendizaje automático sobreajusta la información de entrenamiento. Contrariamente al entendimiento sustancial de la relación con seis amigos, este modelo tiene una percepción profundamente arraigada en los datos de entrenamiento, incluidos los errores y los peculiaridades, de ahí que se desempeñe mal en conjuntos de datos no vistos, justo como su comprensión sobre la vida de los amigos carece de aplicación fuera de los capítulos de \"Friends.\"\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=\"\")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"Constrúyeme una metáfora para explicarme el overfitting con la serie de Friends\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\", # mistralai/Mistral-7B-Instruct-v0.2; google/gemma-2-2b-it\n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
